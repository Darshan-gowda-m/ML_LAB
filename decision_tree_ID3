import pandas as pd
import math
from collections import Counter

# Load dataset
df = pd.read_csv('training_data.csv')
features = list(df.columns[:-1])
target = df.columns[-1]

# Entropy function
def entropy(data):
    labels = data[target]
    total = len(labels)
    counts = Counter(labels)
    return -sum((c/total) * math.log2(c/total) for c in counts.values() if c > 0)

# Information gain
def info_gain(data, attr):
    total_entropy = entropy(data)
    values = data[attr].unique()
    weighted_entropy = sum((len(data[data[attr]==v])/len(data)) * entropy(data[data[attr]==v]) for v in values)
    return total_entropy - weighted_entropy

# ID3 tree builder
def id3(data, features, depth=0):
    labels = data[target]
    if len(set(labels)) == 1:
        return labels.iloc[0]
    if not features:
        return labels.mode()[0]

    gains = {f: info_gain(data, f) for f in features}
    best = max(gains, key=gains.get)
    
    tree = {best: {}}
    for val in data[best].unique():
        subset = data[data[best] == val]
        if subset.empty:
            tree[best][val] = labels.mode()[0]
        else:
            tree[best][val] = id3(subset, [f for f in features if f != best], depth + 1)
    return tree

# Pretty print tree diagram
def print_tree(tree, indent=""):
    if not isinstance(tree, dict):
        print(indent + "→ " + str(tree))
        return
    for attr, branches in tree.items():
        print(indent + f"[{attr}]")
        for val, subtree in branches.items():
            print(indent + f"├── {val}")
            print_tree(subtree, indent + "│   ")

# Classify new sample
def classify(tree, sample):
    if not isinstance(tree, dict):
        return tree
    attr = next(iter(tree))
    value = sample.get(attr)
    subtree = tree[attr].get(value)
    if subtree is None:
        return "Unknown"
    return classify(subtree, sample)

# Accuracy calculation
def compute_accuracy(tree, data):
    correct = 0
    for _, row in data.iterrows():
        sample = row.to_dict()
        actual = sample[target]
        predicted = classify(tree, sample)
        if predicted == actual:
            correct += 1
    return correct / len(data)

# Main execution
print("🔍 Building Decision Tree using ID3 Algorithm...\n")
tree = id3(df, features)

print("📘 Decision Tree Structure:")
print_tree(tree)

accuracy = compute_accuracy(tree, df)
print(f"\n✅ Training Accuracy: {accuracy*100:.2f}%")

# User input to classify
print("\n🧪 Enter values for a new sample to classify:")
new_sample = {}
for f in features:
    new_sample[f] = input(f"{f}: ")

prediction = classify(tree, new_sample)
print(f"\n🔮 Predicted Class: {prediction}")











output
🔍 Building Decision Tree using ID3 Algorithm...

📘 Decision Tree Structure:
[Outlook]
├── Sunny
│   [Temperature]
│   ├── Warm
│   │   [Humidity]
│   │   ├── Normal
│   │   │   → Yes
│   │   ├── High
│   │   │   [Wind]
│   │   │   ├── Strong
│   │   │   │   [Water]
│   │   │   │   ├── Warm
│   │   │   │   │   → No
│   │   │   │   ├── Cool
│   │   │   │   │   → Yes
│   │   │   ├── Weak
│   │   │   │   → No
│   ├── Hot
│   │   → Yes
│   ├── Cold
│   │   → Yes
├── Rainy
│   → No
├── Overcast
│   → Yes

✅ Training Accuracy: 100.00%

🧪 Enter values for a new sample to classify:
Outlook:  Sunny
Temperature:  Hot
Humidity:  Normal
Wind:  Strong
Water:  Cool
Forecast:  Same

🔮 Predicted Class: Yes


training_data.csv
Outlook,Temperature,Humidity,Wind,Water,Forecast,EnjoySport
Sunny,Warm,Normal,Strong,Warm,Same,Yes
Sunny,Warm,High,Strong,Warm,Same,No
Rainy,Cold,High,Strong,Warm,Change,No
Sunny,Warm,High,Strong,Cool,Change,Yes
Overcast,Hot,High,Weak,Cool,Same,Yes
Rainy,Warm,Normal,Weak,Warm,Change,No
Sunny,Hot,Normal,Strong,Warm,Same,Yes
Rainy,Warm,High,Strong,Cool,Change,No
Overcast,Warm,Normal,Weak,Warm,Same,Yes
Sunny,Cold,Normal,Weak,Cool,Same,Yes
Rainy,Hot,High,Strong,Warm,Change,No
Sunny,Hot,High,Weak,Warm,Change,Yes
Sunny,Warm,High,Weak,Cool,Change,No

